{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMN031Z/LuH64xgcXnZFoaw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeerbaniShashi/NLP_all/blob/main/NLP_interview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBkX4aQJOCbh",
        "outputId": "2f60d7f7-00b5-4329-ede4-bfbbce12ad43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "like : like\n",
            "liking : like\n",
            "likely : like\n",
            "like : like\n",
            "likes : like\n"
          ]
        }
      ],
      "source": [
        "# 32. Explain stemming procedure with example.\n",
        "!pip install nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "pst = PorterStemmer()\n",
        "\n",
        "#Choose some words to be stemmed\n",
        "words = [\"like\", \"liking\", \"likely\", \"like\", \"likes\"]\n",
        "for i in words:\n",
        "  print(i, \":\", pst.stem(i))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#33. / 91. Explain Lemmatization procedure with example. / Implement lemmatization using NLTK\n",
        "import nltk\n",
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemma_ti = WordNetLemmatizer()\n",
        "list = [\"cats\", \"dogs\", \"birds\", \"studying,\", \"studing\", \"seeing\", \"laughing\"]\n",
        "for n in list:\n",
        "  print(n + \":\" + lemma_ti.lemmatize(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mASmK8vOwM5",
        "outputId": "3c18cd64-15c8-4068-bdfb-f5685ecdf686"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats:cat\n",
            "dogs:dog\n",
            "birds:bird\n",
            "studying,:studying,\n",
            "studing:studing\n",
            "seeing:seeing\n",
            "laughing:laughing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(\"Enter two comma, separated words\")\n",
        "words = input()\n",
        "\n",
        "tokens = nlp(words)\n",
        "for i in tokens:\n",
        "  print(i.text, i.has_vector, i.vector_norm, i.is_oov)\n",
        "token1, token2 = tokens[0], tokens[1]\n",
        "print(\"Similar:\", token1.similarity(token2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO7jWMW676rk",
        "outputId": "893a6613-dab4-4c40-9bbf-4f7310ac14b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Enter two comma, separated words\n",
            "dogs, cats\n",
            "dogs True 6.2899394 True\n",
            ", True 7.6550837 True\n",
            "cats True 7.1599107 True\n",
            "Similar: -0.05556377395987511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-3141165465.py:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(\"Similar:\", token1.similarity(token2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Enter two comma, separated words\")\n",
        "words = input()\n",
        "\n",
        "tokens = nlp(words)\n",
        "for i in tokens:\n",
        "  print(i.text, i.has_vector, i.vector_norm, i.is_oov)\n",
        "token1, token2 = tokens[0], tokens[1]\n",
        "print(\"Similar:\", token1.similarity(token2))"
      ],
      "metadata": {
        "id": "RjwrV0Uf76y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a3a46c-4754-4ea6-96e6-08af5d23348e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter two comma, separated words\n",
            "orange, apple\n",
            "orange True 6.1769333 True\n",
            ", True 7.612471 True\n",
            "apple True 5.8377614 True\n",
            "Similar: 0.014337177388370037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-303310194.py:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(\"Similar:\", token1.similarity(token2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "Text = \"Google is the most common way to get your answer\"\n",
        "document = nlp(Text)\n",
        "for ent in document.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwgdWyb99FZ9",
        "outputId": "3a320a33-9232-4a19-8988-e81c6191dcbf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google 0 6 ORG\n"
          ]
        }
      ]
    }
  ]
}