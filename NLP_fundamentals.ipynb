{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7uBm5IuPVkFdyZcwDyKXr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeerbaniShashi/NLP_all/blob/main/NLP_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TugOLWg0DdFC",
        "outputId": "c7bf439d-fafc-47f0-b722-2e2eb494fa4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Welcome to the Great Learning.', 'I am very happy to be part of the Great Learning.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#Line tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download punkt_tab resource\n",
        "\n",
        "data = \"Welcome to the Great Learning. I am very happy to be part of the Great Learning.\"\n",
        "tokens = nltk.sent_tokenize(data)\n",
        "print(tokens)\n",
        "\n",
        "type(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQR-SRbiEJdR",
        "outputId": "ea002a38-73fe-4014-b38f-bd987fac7b8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word tokenization\n",
        "tokens = nltk.word_tokenize(data)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZgfWXiWELXr",
        "outputId": "dbf11df0-9d4b-4985-d252-044b8da0a69f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Welcome', 'to', 'the', 'Great', 'Learning', '.', 'I', 'am', 'very', 'happy', 'to', 'be', 'part', 'of', 'the', 'Great', 'Learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sah259hELgQ",
        "outputId": "1dc6fbe5-ab43-48cd-f16c-8b3a1a603a52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "BocpUciZEW_E",
        "outputId": "d89a2d07-35c0-4486-b819-fe0046f8e36a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "#import these modules\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "portstem = PorterStemmer()\n",
        "\n",
        "#Choose some words to be stemmed\n",
        "words = [\"Like\", \"Liking\", \"Likes\"]\n",
        "\n",
        "for i in words:\n",
        "  print(i, \" : \", portstem.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR8RS6Icxjc4",
        "outputId": "eb4c4687-139c-444b-8f21-0520bd492a33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Like  :  like\n",
            "Liking  :  like\n",
            "Likes  :  like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRFHcRBhxkRc",
        "outputId": "9995ee9a-f4b3-45d9-b5b5-319748b10f8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import these modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmati = WordNetLemmatizer()\n",
        "\n",
        "print(\"socks :\", lemmati.lemmatize(\"socks\"))\n",
        "print(\"sons :\", lemmati.lemmatize(\"sons\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjeOgFfzxkhS",
        "outputId": "42d67dea-290b-406a-8395-5b8463a51e71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "socks : sock\n",
            "sons : son\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "data = \"\"\"Data science is one of the most trendind field to work with. It needs data to give prediction by using the past scenarios\"\"\"\n",
        "\n",
        "stop_word = set(stopwords.words('english'))\n",
        "\n",
        "print(stopwords.words() [960 : 1020])"
      ],
      "metadata": {
        "id": "glD2vHLuxkij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0172a7f-20ad-4846-fd5d-7680de798c44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّa', 'ad', 'altı', 'altmış', 'amma', 'arasında', 'artıq', 'ay', 'az', 'bax', 'belə', 'bəli', 'bəlkə', 'beş', 'bəy', 'bəzən', 'bəzi', 'bilər', 'bir', 'biraz', 'biri', 'birşey', 'biz', 'bizim', 'bizlər', 'bu', 'buna', 'bundan', 'bunların', 'bunu']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ3vQVRgxozK",
        "outputId": "ba0db86e-e577-46cd-d65a-b17b100bd2dc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = nltk.word_tokenize(data)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYlapzlLxvru",
        "outputId": "5311b695-c5b3-4341-bd52-7661814e53fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'science',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'trendind',\n",
              " 'field',\n",
              " 'to',\n",
              " 'work',\n",
              " 'with',\n",
              " '.',\n",
              " 'It',\n",
              " 'needs',\n",
              " 'data',\n",
              " 'to',\n",
              " 'give',\n",
              " 'prediction',\n",
              " 'by',\n",
              " 'using',\n",
              " 'the',\n",
              " 'past',\n",
              " 'scenarios']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "for word in data:\n",
        "  if word in stops:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akQpRCTJx2g7",
        "outputId": "24243cb9-28d0-4ec5-cf7a-e2fc42c7c70e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is\n",
            "of\n",
            "the\n",
            "most\n",
            "to\n",
            "with\n",
            "to\n",
            "by\n",
            "the\n"
          ]
        }
      ]
    }
  ]
}